# Pelican
Toward Efficient Sequence Modeling with Temperature assisted
Information Gating
Novel Alternative to Self Attention


# i have added jupyter notebook you can check the code there 

# You can also check my Research Paper 
Self-attention mechanisms, popularized by the Transformer architecture , have significantly advanced the field of deep learning. Despite their strengths in modeling global dependencies, they face

challenges such as high computational costs and inefficiencies in processing long sequences. This paper

offers a critical evaluation of self-attention and introduces a novel approach â€” [Temperature assisted

information Gating] to surpass Self Attention efficiently and overcoming these issues.



