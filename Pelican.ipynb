{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"3jNqAo_vYlHT","executionInfo":{"status":"ok","timestamp":1747884824102,"user_tz":-300,"elapsed":30,"user":{"displayName":"Muhammad Hamza Alvi","userId":"14962369808426805467"}}},"outputs":[],"source":["def flush():\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","def count_parameters(model):\n","    return f\"BabyLlama size: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 10 ** 6:.2f}M parameters\""]},{"cell_type":"code","source":[],"metadata":{"id":"PPAG1hOrDTP3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now Restart Runtime"],"metadata":{"id":"jwWY37a5DT_N"}},{"cell_type":"code","source":["!pip install -U -q accelerate transformers[torch] datasets huggingface_hub torchviz"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYpycAjIC1mK","executionInfo":{"status":"ok","timestamp":1747884768927,"user_tz":-300,"elapsed":103721,"user":{"displayName":"Muhammad Hamza Alvi","userId":"14962369808426805467"}},"outputId":"bccce8cc-a6f2-409d-a058-74b110cd2f80"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.3/489.3 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install hf_xet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"roEHMfiMC4VK","executionInfo":{"status":"ok","timestamp":1747884771980,"user_tz":-300,"elapsed":3068,"user":{"displayName":"Muhammad Hamza Alvi","userId":"14962369808426805467"}},"outputId":"5da1827e-4746-4bf4-8b4f-6b22dc1faa24"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting hf_xet\n","  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n","Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/5.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/5.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: hf_xet\n","Successfully installed hf_xet-1.1.2\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"bZjKLwgz6os2","colab":{"base_uri":"https://localhost:8080/","height":389,"referenced_widgets":["aecb9279ecd74aafa31b6b8f57fd4ff2","c466920a5fae4710929db80b7d456caa","01a715de61ac44779ea9c7d97eccdeb4","59b183a66cd0487299b8ab00b400cd3a","1b21f5c540c743649a0042c203b75cf9","49154b633fb444f49b5826e022589ce0","21ac4b70caf04d728263fa9df3aed229","abaa328e66e4400a86754aeeaf3af1b6","042cd29ecd6042818b3a4ebb58870780","cd335f24f7104654ad1eb5f0be6ce844","060c4b378cb74e0c91a4a54697b1005f","b6a6ea627d8a4401862ca753c930eaef","38b414f3b9d44261be50a566ce05b849","d40b1a6666bb4681a8cd62a2c0ea3d8c","d3228e4342c74df5a7f7b024a66e9157","d7fbc0b9a9f84138b1d5972a9ffae194","57c5ed4e1a084cb0a6836f710688d586","3efcc89216aa40c68a75bf79d1659e86","ec5851c5e153443984db0cb03803dfc5","c3ef2715cf044533a5c78c23fde4a953","bed17c0b01074668b27264e2abe8143f","390bab03d1e042a78b05e09ff383e3a7","c4d0ae4c3f92472c90e9f582d3520cdc","d26bfb58474b4173ad7ba48b40f7724c","dc14534addee42a1bd48f78b13e73c84","278831e2257c4368b46bcebec3fc78eb","f16050227c4440cfb8b8a0046dcbf39a","0ea3b908b0f84fafabee1bcb22d12762","3e03e2b078de46d59f3078fcfff6b18a","67889ae4c02544a8857f15e03af3c390","de7e639de1fd4d14b3df948c3508e1ce","d60e1a60764a48c8b1e128a34fc72d4e","8d742e2206744c65964c0de5e96be4b4","c5c9ff192fbf4d3095f13ecdf18e2393","ae0de8c0e034471bb9cf0ba582e551ec","00e8910f42fc431cbebf7e355db87991","973305381631464abe1c978e5fe3998e","9403ebfa60244fd598d0bb37ae5557f6","7e53713e763a4465bad2b3d05b9ff25a","ee8d4480ff8e462da2c647cb475eee88","b89b27800e8b4090ad1f0b7093663350","b1e846d7c7a2495096be8de78e582d77","3864769ba542427898d1556b2e165290","75bdb24d509b4929b2ae26d9829bb271","cbd0d60cecd442499c1ad72f2e589a27","866e467bbec042a6bbe77201534dcfee","d6114c9d791640fb8c766916652c7e97","e665d19376fa44fab233b8c26a1ff74f","bf7824a9765a4966a78422ec5f070703","5089628c9cfa46adb27d85e29d241ce6","ad6ef57b65de47778f5152edc3dc350b","09d752425a2a4f1597456e62ce03b083","97a6a19c97294ea4912801e28022f99b","7582aa6f226842ca8ff095f350c06b08","90eded1b625647a68be6cff1755abf09","9f2f75343f454d32a121ab3262c0197a","77103bafa8bf4d5096172e5f4407532a","4a69e9288fc74d788da9c35e104e4583","6326ab28d39f432884a26e25e8e107c6","5573eb47e4a94f89bebfd17dc158cd4d","3bf19b46e09b46f594cd3987c77f6e7a","df831808664841ce9ba6059b6785712e","67ed0674692f4afdb1c578cf2d7ce401","19289a1a8a7c4d5686e1fc025e6fb5c9","daa65961ee1e45b08a01511508468b1e","0c1fec5dd83249f4940ad946a3aef038"]},"executionInfo":{"status":"ok","timestamp":1747884861079,"user_tz":-300,"elapsed":28948,"user":{"displayName":"Muhammad Hamza Alvi","userId":"14962369808426805467"}},"outputId":"23910f6f-c1a0-4131-f3b1-7b48bf555796"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/944 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aecb9279ecd74aafa31b6b8f57fd4ff2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["data/train-00000-of-00002.parquet:   0%|          | 0.00/153M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6a6ea627d8a4401862ca753c930eaef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["data/train-00001-of-00002.parquet:   0%|          | 0.00/153M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d0ae4c3f92472c90e9f582d3520cdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c9ff192fbf4d3095f13ecdf18e2393"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/3.59k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd0d60cecd442499c1ad72f2e589a27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/7.85M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f2f75343f454d32a121ab3262c0197a"}},"metadata":{}}],"source":["import gc\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","from math import sqrt\n","from transformers import PretrainedConfig\n","import math\n","from typing import Tuple, Optional, List\n","from transformers import logging, PreTrainedModel\n","from transformers.modeling_outputs import CausalLMOutputWithPast\n","from datasets import load_dataset\n","from transformers import AutoTokenizer\n","\n","dataset = load_dataset(\"HuggingFaceTB/cosmopedia-100k\")\n","tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")  # Use Hugging Face tokenizer or your own\n","\n","\n","logger = logging.get_logger(__name__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eEToaqO0ATJs"},"outputs":[],"source":["ds = dataset[\"train\"][\"text\"][5]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t1MikAY3ATJv"},"outputs":[],"source":["ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xf2Oy7eGLr8f"},"outputs":[],"source":["class LlamaConfig(PretrainedConfig):\n","    model_type = \"llama\"\n","    keys_to_ignore_at_inference = [\"past_key_values\"]\n","    def __init__(\n","        self,\n","        vocab_size=128000,\n","        hidden_size=192, # 2048 Tiny LLaMA\n","        intermediate_size=512,\n","        num_hidden_layers=12,\n","        num_attention_heads=4, # 32 Tiny LLaMA\n","        num_key_value_heads=2,\n","        hidden_act=\"silu\",\n","        max_position_embeddings=128,\n","        initializer_range=0.02,\n","        rms_norm_eps=1e-6,\n","        use_cache=False,\n","        pad_token_id=None,\n","        bos_token_id=1,\n","        eos_token_id=2,\n","        pretraining_tp=1,\n","        tie_word_embeddings=False,\n","        rope_theta=10000.0,\n","        rope_scaling=None,\n","        attention_bias=False,\n","        attention_dropout=0.0,\n","        use_bias=False,\n","        lm_head_bias=False,\n","        residual_dropout=0.0,\n","        device='cpu',\n","        **kwargs,\n","    ):\n","\n","        self.vocab_size = vocab_size\n","        self.max_position_embeddings = max_position_embeddings\n","        self.hidden_size = hidden_size\n","        self.intermediate_size = intermediate_size\n","        self.num_hidden_layers = num_hidden_layers\n","        self.num_attention_heads = num_attention_heads\n","\n","        self.num_key_value_heads = num_key_value_heads\n","        self.hidden_act = hidden_act\n","        self.initializer_range = initializer_range\n","        self.rms_norm_eps = rms_norm_eps\n","        self.pretraining_tp = pretraining_tp\n","        self.use_cache = use_cache\n","        self.rope_theta = rope_theta\n","        self.rope_scaling = rope_scaling\n","        self.attention_bias = attention_bias\n","        self.attention_dropout = attention_dropout\n","        self.residual_dropout = residual_dropout\n","        self.use_bias = use_bias\n","        self.lm_head_bias = lm_head_bias\n","        self.device = device\n","\n","        super().__init__(\n","            bos_token_id=bos_token_id,\n","            eos_token_id=eos_token_id,\n","            **kwargs,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SCgkazpgT7h6"},"outputs":[],"source":["def build_mask_cache(max_seq_length: int, device: Optional[torch.device] = None) -> torch.Tensor:\n","    ones = torch.ones((max_seq_length, max_seq_length), device=device, dtype=torch.bool)\n","    return torch.tril(ones).unsqueeze(0).unsqueeze(0)\n","\n","def repeat_kv(hidden_states:torch.tensor, n_repeats:int):\n","    batch, n_kv_heads, seq_len, head_dim = hidden_states.shape\n","    if n_repeats == 1:\n","        return hidden_states\n","    hidden_states = hidden_states.unsqueeze(2).expand(batch, n_kv_heads, n_repeats, seq_len, head_dim) # (B, nh, T, hs) -> (B, nh, 1, T, hs) -> # (B, nh, n_repeats, T, hs)\n","    return hidden_states.reshape(batch, n_kv_heads * n_repeats, seq_len, head_dim) # # (B, nh * n_repeats, T, hs)\n","\n","\n","class RotaryPositionalEmbeddings(nn.Module):\n","    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n","        super().__init__()\n","        self.dim = dim\n","        self.max_position_embeddings = max_position_embeddings\n","        self.device=device\n","        self.scaling_factor = scaling_factor\n","        self.base = base\n","        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n","        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n","\n","        # Build here to make `torch.jit.trace` work.\n","        self._set_cos_sin_cache(\n","            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n","        )\n","\n","    def _set_cos_sin_cache(self, seq_len, device, dtype):\n","        self.max_seq_len_cached = seq_len\n","        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n","        t = t / self.scaling_factor\n","        freqs = torch.outer(t, self.inv_freq)\n","        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n","        emb = torch.cat((freqs, freqs), dim=-1)\n","        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n","        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n","\n","    @torch.no_grad()\n","    def forward(self, x, seq_len=None):\n","        # x: [bs, num_attention_heads, seq_len, head_size]\n","        if seq_len > self.max_seq_len_cached:\n","            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n","\n","        return (\n","            self.cos_cached[:seq_len].to(dtype=x.dtype),\n","            self.sin_cached[:seq_len].to(dtype=x.dtype),\n","        )\n","\n","    def apply_rope(self, x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor, position_ids, unsqueeze_dim=1) -> torch.Tensor:\n","        cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n","        sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n","        x1 = x[..., : x.shape[-1] // 2] # (B, nh, T, hs/2)\n","        x2 = x[..., x.shape[-1] // 2 :] # (B, nh, T, hs/2)\n","        rotated = torch.cat((-x2, x1), dim=-1) # (B, nh, T, hs)\n","        roped = (x * cos) + (rotated * sin)\n","        return roped.to(dtype=x.dtype)\n","\n","\n","    @property\n","    def sin_cached(self):\n","\n","        return self._sin_cached\n","\n","    @property\n","    def cos_cached(self):\n","\n","        return self._cos_cached\n","\n","\n","class KVCache(nn.Module):\n","    def __init__(\n","        self,\n","        k_shape: Tuple[int, int, int, int],\n","        v_shape: Tuple[int, int, int, int],\n","        device: Optional[torch.device] = None,\n","        dtype: Optional[torch.dtype] = None,\n","    ) -> None:\n","        super().__init__()\n","        self.register_buffer(\"k\", torch.zeros(k_shape, device=device, dtype=dtype), persistent=False)\n","        self.register_buffer(\"v\", torch.zeros(v_shape, device=device, dtype=dtype), persistent=False)\n","\n","    def forward(self, input_pos: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n","        # move the buffer to the activation dtype for when AMP is used\n","        self.k = self.k.to(k.dtype)\n","        self.v = self.v.to(v.dtype)\n","        # update the cache\n","        k = self.k.index_copy_(2, input_pos, k)\n","        v = self.v.index_copy_(2, input_pos, v)\n","        return k, v\n","\n","    def reset_parameters(self) -> None:\n","        torch.nn.init.zeros_(self.k)\n","        torch.nn.init.zeros_(self.v)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9D3EVPKaYXRJ"},"outputs":[],"source":["class LlamaAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.hidden_dim = hidden_dim = config.hidden_size\n","        self.n_heads = n_heads = config.num_attention_heads\n","        self.n_kv_heads = n_kv_heads = config.num_key_value_heads\n","        self.head_dim = head_dim = config.hidden_size // n_heads\n","        use_bias = config.use_bias\n","\n","        if (head_dim * n_heads) != self.hidden_dim:\n","            raise ValueError(\n","                f\"hidden_dim must be divisible by num_heads (got `hidden_dim`: {self.hidden_dim}\"\n","                f\" and `num_heads`: {self.n_heads}).\"\n","            )\n","\n","        self.repeats = n_heads // n_kv_heads # q_per_kv\n","\n","        self.q_proj = nn.Linear(hidden_dim, n_heads * head_dim, bias=use_bias)\n","        self.k_proj = nn.Linear(hidden_dim, n_kv_heads * head_dim, bias=use_bias)\n","        self.v_proj = nn.Linear(hidden_dim, n_kv_heads * head_dim, bias=use_bias)\n","        self.o_proj = nn.Linear(n_heads * head_dim, hidden_dim, bias=use_bias)\n","\n","        self.rotary_emb = RotaryPositionalEmbeddings(\n","            head_dim,\n","            max_position_embeddings=config.max_position_embeddings,\n","            device=config.device,\n","            base=config.rope_theta,\n","        )\n","\n","        self.kv_cache: Optional[KVCache] = None\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        mask: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","    ):\n","        B, T, _ = hidden_states.size() # bsz, seq_len, embed_dim\n","\n","        queries = self.q_proj(hidden_states)\n","        keys = self.k_proj(hidden_states)\n","        values = self.v_proj(hidden_states)\n","\n","\n","        queries = queries.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)  # bsz, seq_len, n_heads, head_dim\n","        keys = keys.view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2) # bsz, seq_len, n_kv_heads, head_dim\n","        values = values.view(B, T, self.n_kv_heads, self.head_dim).transpose(1, 2)\n","\n","        kv_seq_len = keys.shape[-2]\n","        cos, sin = self.rotary_emb(values, seq_len=kv_seq_len)\n","\n","        queries = self.rotary_emb.apply_rope(queries, cos, sin, position_ids)\n","        keys = self.rotary_emb.apply_rope(keys, cos, sin, position_ids)\n","\n","\n","        # TODO: KV caching\n","        keys = repeat_kv(keys, self.repeats)\n","        values = repeat_kv(values, self.repeats)\n","\n","        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n","        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n","        if queries.device.type == \"cuda\" and mask is not None:\n","            queries = queries.contiguous()\n","            keys = keys.contiguous()\n","            values = values.contiguous()\n","\n","        y = self.scaled_dot_product_attention(queries, keys, values, mask) # (B, T, n_heads, head_dim)\n","\n","        y = y.reshape(B, T, self.hidden_dim) # (B, T, hidden_dim)\n","\n","        return self.o_proj(y)\n","\n","\n","    def scaled_dot_product_attention(\n","        self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, mask: Optional[torch.Tensor] = None\n","    ) -> torch.Tensor:\n","\n","        print()\n","        scale = 1.0 / math.sqrt(self.head_dim)\n","        y = torch.nn.functional.scaled_dot_product_attention(\n","            q, k, v, attn_mask=None, dropout_p=0.0, scale=scale, is_causal=True\n","        )\n","        return y.transpose(1, 2).contiguous()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQfC90HwjtgN"},"outputs":[],"source":["class LLaMAMLP(nn.Module):\n","    def __init__(self, hidden_dim, intermediate_dim): # in MLP: intermediate_dim= 4 * hidden_dim\n","        super(LLaMAMLP, self).__init__()\n","        self.linear_1 = nn.Linear(hidden_dim, intermediate_dim)\n","        self.linear_2 = nn.Linear(hidden_dim, intermediate_dim) # Original: intermediate -> hidden.\n","        self.activation_fn = nn.SiLU()\n","        self.out_proj = nn.Linear(intermediate_dim, hidden_dim) # Original: dropout\n","\n","\n","    def forward(self, hidden_states):\n","        x_fc_1 = self.linear_1(hidden_states)\n","        x_fc_2 = self.linear_2(hidden_states)\n","        x = self.activation_fn(x_fc_1) * x_fc_2\n","        x = self.out_proj(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DDnXHH6bP4a3"},"outputs":[],"source":["class LlamaRMSNorm(nn.Module):\n","    def __init__(self, hidden_size, eps=1e-6):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(hidden_size))\n","        self.variance_epsilon = eps\n","\n","    def forward(self, hidden_states):\n","        input_dtype = hidden_states.dtype\n","        hidden_states = hidden_states.to(torch.float32)\n","        variance = hidden_states.pow(2).mean(-1, keepdim=True) # (1/n) * Σ x_i^2\n","        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n","        return self.weight * hidden_states.to(input_dtype)\n","\n","class Block(nn.Module):\n","    def __init__(self, config: LlamaConfig):\n","        super(Block, self).__init__()\n","        self.hidden_dim = hidden_dim = config.hidden_size\n","        self.intermediate_dim = intermediate_dim = config.intermediate_size\n","\n","        self.attn = LlamaAttention(config)\n","\n","        self.mlp = LLaMAMLP(hidden_dim, intermediate_dim)\n","        self.input_layernorm = LlamaRMSNorm(hidden_dim, eps=config.rms_norm_eps)\n","        self.post_attention_layernorm = LlamaRMSNorm(hidden_dim, eps=config.rms_norm_eps)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        mask: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.LongTensor] = None,\n","    ):\n","        r = self.attn(self.input_layernorm(hidden_states), mask,position_ids,)\n","        h = hidden_states + r\n","        r = self.mlp(self.post_attention_layernorm(h))\n","        out = h + r\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GcngjpHCSUnh"},"outputs":[],"source":["class LlamaModel(nn.Module):\n","    def __init__(self, config):\n","        super(LlamaModel, self).__init__()\n","        self.config = config\n","        self.hidden_dim = hidden_dim = config.hidden_size\n","        self.vocab_size = vocab_size = config.vocab_size\n","        assert self.vocab_size > 0\n","        self.num_hidden_layers = num_hidden_layers = config.num_hidden_layers\n","        self.embed = nn.Embedding(vocab_size,10)\n","\n","        self.embed_ln = nn.Linear(10,hidden_dim,bias=False)\n","        self.blocks = nn.ModuleList(\n","            [Block(config) for _ in range(num_hidden_layers)]\n","        )\n","        self.norm = LlamaRMSNorm(hidden_dim, eps=config.rms_norm_eps)\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        mask: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.Tensor] = None,\n","    ):\n","\n","        x = self.embed(hidden_states)\n","        x = self.embed_ln(x)\n","\n","        seq_len = hidden_states.size(1)\n","        if position_ids is None:\n","            position_ids = torch.arange(seq_len, dtype=torch.long, device=self.config.device).unsqueeze(0)\n","\n","\n","        for b in self.blocks:\n","            x = b(x, mask, position_ids)\n","\n","        return self.norm(x)\n","\n","\n","class LlamaPreTrainedModel(PreTrainedModel):\n","    config_class = LlamaConfig\n","    base_model_prefix = \"model\"\n","    supports_gradient_checkpointing = True\n","    _skip_keys_device_placement = \"past_key_values\"\n","\n","    def _init_weights(self, module):\n","        std = self.config.initializer_range\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=std)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=std)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","\n","\n","class LlamaForCausalLM(LlamaPreTrainedModel):\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.model     = LlamaModel(config)\n","        self.lm_head = nn.Linear(config.hidden_size, 128000, bias=False)\n","        self.post_init()\n","        self.lm_head.weight.data.fill_(0)\n","\n","    def forward(\n","        self,\n","        input_ids:      torch.Tensor = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        position_ids:   Optional[torch.Tensor] = None,\n","        labels:         Optional[torch.Tensor] = None,\n","    ):\n","\n","        outputs = self.model(\n","            hidden_states=input_ids,\n","            mask=attention_mask,\n","            position_ids=position_ids,\n","        )\n","\n","        x    = outputs\n","\n","        B,T,C = x.shape\n","        logits = self.lm_head(x)\n","\n","\n","        loss = None\n","        if labels is not None:\n","\n","            # shift so that tokens < n predict n\n","            shift_logits = logits[...,:-1, :].contiguous()# all elements expect the last one\n","            shift_labels = labels[...,1:].contiguous() # all elements except the first\n","\n","            # Flatten the tokens\n","            shift_logits = shift_logits.view(-1,128000)\n","            #shift_logits = shift_logits.view(-1, self.config.vocab_size)\n","\n","            shift_labels = shift_labels.view(-1)\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(shift_logits, shift_labels)\n","\n","        return CausalLMOutputWithPast(\n","            loss=loss,\n","            logits=logits,\n","        )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZqvBWtmVxUk"},"outputs":[],"source":["device=\"cpu\"\n","config = LlamaConfig(device=device)\n","llm = LlamaForCausalLM(config).to(config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUnuoPINY9VS"},"outputs":[],"source":["\n","count_parameters(llm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"geZxTDtrYica"},"outputs":[],"source":["llm"]},{"cell_type":"markdown","metadata":{"id":"LDRhgXQ66Wd9"},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIizZ1th7vo0"},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92zkJa1EATKV"},"outputs":[],"source":["tokenized_datasets = dataset.map(\n","    lambda examples: tokenizer(\n","        examples[\"text\"],\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=confi.max_position_embedding,\n","    ),\n","    batched=True,\n",")\n","\n","# Data Collator\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=False  # Set mlm=False for autoregressive models\n",")"]},{"cell_type":"markdown","metadata":{"id":"wZJtxpH86T1M"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3Ik3FsQNRh1"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","args = TrainingArguments(\n","    \"./Llama/\",\n","    per_device_train_batch_size=32,\n","    #max_steps=150,\n","    num_train_epochs=2,\n","    logging_steps=5,\n","    #save_strategy = \"\"\n","    #resume_from_checkpoint = True,\n","    #gradient_accumulation_steps=2,\n","    #weight_decay=0.1,\n","    #warmup_steps= 1_000,\n","    #lr_scheduler_type=\"linear\",\n","    learning_rate=0.001,\n","    #save_steps=500,\n","    fp16=True,\n","    report_to = \"none\",\n","    #torch_compile = True,\n","    push_to_hub=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lAlbvx8-OITv"},"outputs":[],"source":["trainer = Trainer(\n","    model=llm,\n","    tokenizer=tokenizer,\n","    args=args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets[\"train\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJSfGx2I2Io6"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"iPBP9AjkATKd"},"source":["Now Traininig Pelican"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wNY1uCPMATKf"},"outputs":[],"source":["dell llm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PEPVkNdKATKh"},"outputs":[],"source":["class LLaMAMLP(nn.Module):\n","    def __init__(self, hidden_dim, intermediate_dim): # in MLP: intermediate_dim= 4 * hidden_dim\n","        super(LLaMAMLP, self).__init__()\n","        self.linear_1 = nn.Linear(hidden_dim, intermediate_dim,bias=False)\n","        self.linear_2 = nn.Linear(hidden_dim, intermediate_dim,bias=False) # Original: intermediate -> hidden.\n","        self.activation_fn = nn.SiLU()\n","        self.out_proj = nn.Linear(intermediate_dim, hidden_dim,bias=False) # Original: dropout\n","\n","\n","    def forward(self, hidden_states):\n","        hidden_states = hidden_states\n","        x_fc_1 = self.linear_1(hidden_states)\n","        x_fc_2 = self.linear_2(hidden_states)\n","        x = self.activation_fn(x_fc_1) * x_fc_2\n","        x = self.out_proj(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vezczNJ0ATKi"},"outputs":[],"source":["class LlamaRMSNorm(nn.Module):\n","    def __init__(self, hidden_size, eps=1e-6):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(hidden_size))\n","        self.variance_epsilon = eps\n","\n","    def forward(self, hidden_states):\n","        input_dtype = hidden_states.dtype\n","        hidden_states = hidden_states.to(torch.float32)\n","        variance = hidden_states.pow(2).mean(-1, keepdim=True) # (1/n) * Σ x_i^2\n","        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n","        return self.weight * hidden_states.to(input_dtype)\n","\n","class Block(nn.Module):\n","    def __init__(self, config: LlamaConfig):\n","        super(Block, self).__init__()\n","        self.hidden_dim = hidden_dim = config.hidden_size\n","        self.intermediate_dim = intermediate_dim = config.intermediate_size\n","        self.mlp = LLaMAMLP(hidden_dim, intermediate_dim)\n","        self.input_layernorm = LlamaRMSNorm(hidden_dim, eps=config.rms_norm_eps)\n","        self.post_attention_layernorm = LlamaRMSNorm(hidden_dim, eps=config.rms_norm_eps)\n","\n","    def forward(\n","        self,\n","        hidden_states,\n","        mask: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.Tensor] = None,\n","    ):\n","\n","        r = hidden_states\n","        h = hidden_states\n","        r = self.mlp(self.post_attention_layernorm(h))\n","        out = h  + r\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S44w4fAoATKl"},"outputs":[],"source":["class PelicanModel(nn.Module):\n","    def __init__(self, config):\n","        super(PelicanModel, self).__init__()\n","        self.config = config\n","        self.hidden_dim = hidden_dim = config.hidden_size\n","        self.vocab_size = vocab_size = config.vocab_size\n","        assert self.vocab_size > 0\n","        self.num_hidden_layers = num_hidden_layers = config.num_hidden_layers\n","        self.embed = nn.Embedding(vocab_size,10)\n","\n","        self.embed_ln = nn.Linear(10,hidden_dim,bias=False)\n","        self.blocks = nn.ModuleList(\n","            [Block(config) for _ in range(num_hidden_layers)]\n","        )\n","        self.norm = LlamaRMSNorm(hidden_dim, eps=config.rms_norm_eps)\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        mask: Optional[torch.Tensor] = None,\n","        position_ids: Optional[torch.Tensor] = None,\n","    ):\n","\n","        x = self.embed(hidden_states)\n","        x = self.embed_ln(x)\n","\n","        seq_len = hidden_states.size(1)\n","        if position_ids is None:\n","            position_ids = torch.arange(seq_len, dtype=torch.long, device=self.config.device).unsqueeze(0)\n","\n","        for b in self.blocks:\n","            x = b(x, mask, position_ids)\n","\n","        return self.norm(x)\n","\n","class PelicanPreTrainedModel(PreTrainedModel):\n","    config_class = LlamaConfig\n","    base_model_prefix = \"model\"\n","    supports_gradient_checkpointing = True\n","    _skip_keys_device_placement = \"past_key_values\"\n","\n","    def _init_weights(self, module):\n","        std = self.config.initializer_range\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean=0.0, std=std)\n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=std)\n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","\n","\n","class PelicanForCausalLM(PelicanPreTrainedModel):\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.model     = PelicanModel(config)\n","        self.lm_head   = nn.Linear(config.hidden_size,128000,bias=False)\n","        self.post_init()\n","        self.lm_head.weight.data.fill_(0)\n","\n","    def forward(\n","        self,\n","        input_ids:      torch.Tensor = None,\n","        attention_mask: Optional[torch.Tensor] = None,\n","        position_ids:   Optional[torch.Tensor] = None,\n","        labels:         Optional[torch.Tensor] = None,\n","    ):\n","\n","        outputs = self.model(\n","            hidden_states=input_ids,\n","            mask=attention_mask,\n","            position_ids=position_ids,\n","        )\n","\n","        x  = outputs\n","\n","        B,T,C = x.shape\n","\n","        x = x/0.1\n","        logits = self.lm_head(x)\n","\n","\n","        loss = None\n","        if labels is not None:\n","\n","            # shift so that tokens < n predict n\n","            shift_logits = logits[...,:-1, :].contiguous()# all elements expect the last one\n","\n","            shift_labels = labels[...,1:].contiguous() # all elements except the first\n","\n","            # Flatten the tokens\n","            shift_logits = shift_logits.view(-1,128000)\n","            #shift_logits = shift_logits.view(-1, self.config.vocab_size)\n","\n","            shift_labels = shift_labels.view(-1)\n","            loss_fct = nn.CrossEntropyLoss()\n","\n","            loss = loss_fct(shift_logits, shift_labels)\n","\n","        return CausalLMOutputWithPast(\n","            loss=loss,\n","            logits=logits,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1QFsGrYATKp"},"outputs":[],"source":["device=\"cpu\"\n","config = LlamaConfig(device=device)\n","llm = PelicanForCausalLM(config).to(config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEdiyql5ATKr"},"outputs":[],"source":["llm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OxfJQBBlATKt"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","\n","args = TrainingArguments(\n","    \"./Llama/\",\n","    per_device_train_batch_size=2,\n","    #max_steps=150,\n","    num_train_epochs=2,\n","    logging_steps=5,\n","    #save_strategy = \"\"\n","    #resume_from_checkpoint = True,\n","    #gradient_accumulation_steps=2,\n","    #weight_decay=0.1,\n","    #warmup_steps= 1_000,\n","    #lr_scheduler_type=\"linear\",\n","    learning_rate=0.001,\n","    #save_steps=500,\n","    fp16=True,\n","    report_to = \"none\",\n","    #torch_compile = True,\n","    push_to_hub=False,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_UFYneHATKv"},"outputs":[],"source":["trainer = Trainer(\n","    model=llm,\n","    tokenizer=tokenizer,\n","    args=args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_datasets[\"train\"]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbSEa4iIATKx"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdQkub6KATKz"},"outputs":[],"source":["\n","\n","tokens = tokenizer(\n","    \" once upon a time in  \",\n","\n","\n","    return_tensors='pt'\n",")#.to('cuda')\n","input_ids = tokens['input_ids']\n","\n","temperature = 1\n","top_k = None\n","top_p = None\n","\n","# Generate the tokens one by one\n","for _ in range(40):\n","    # Get the logits from the model\n","    outputs = llm(input_ids)\n","    logits = outputs.logits[:, -1, :]\n","    # Apply temperature scaling\n","    logits = logits / temperature\n","\n","    # Apply top-k or top-p sampling if specified\n","    if top_k is not None:\n","        logits = logits.topk(top_k, dim=-1)[0]\n","    elif top_p is not None:\n","        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","        sorted_indices_to_remove = cumulative_probs > top_p\n","        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n","        sorted_indices_to_remove[:, 0] = 0\n","        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n","        logits = logits.masked_fill(indices_to_remove, -float('inf'))\n","\n","    # Sample the next token from the logits\n","    next_token_id = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n","\n","    # Update the input with the new token\n","    input_ids = torch.cat([input_ids, next_token_id], dim=-1)\n","\n","# Decode the generated text\n","generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n","print(generated_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WOTMrjpHATK2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZj8UgzIATK4"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1HNol2SKjC9ERZcthWGDP3B7xEQD8uZFG","timestamp":1747884922804}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"aecb9279ecd74aafa31b6b8f57fd4ff2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c466920a5fae4710929db80b7d456caa","IPY_MODEL_01a715de61ac44779ea9c7d97eccdeb4","IPY_MODEL_59b183a66cd0487299b8ab00b400cd3a"],"layout":"IPY_MODEL_1b21f5c540c743649a0042c203b75cf9"}},"c466920a5fae4710929db80b7d456caa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49154b633fb444f49b5826e022589ce0","placeholder":"​","style":"IPY_MODEL_21ac4b70caf04d728263fa9df3aed229","value":"README.md: 100%"}},"01a715de61ac44779ea9c7d97eccdeb4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_abaa328e66e4400a86754aeeaf3af1b6","max":944,"min":0,"orientation":"horizontal","style":"IPY_MODEL_042cd29ecd6042818b3a4ebb58870780","value":944}},"59b183a66cd0487299b8ab00b400cd3a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd335f24f7104654ad1eb5f0be6ce844","placeholder":"​","style":"IPY_MODEL_060c4b378cb74e0c91a4a54697b1005f","value":" 944/944 [00:00&lt;00:00, 91.7kB/s]"}},"1b21f5c540c743649a0042c203b75cf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49154b633fb444f49b5826e022589ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21ac4b70caf04d728263fa9df3aed229":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abaa328e66e4400a86754aeeaf3af1b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"042cd29ecd6042818b3a4ebb58870780":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd335f24f7104654ad1eb5f0be6ce844":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"060c4b378cb74e0c91a4a54697b1005f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b6a6ea627d8a4401862ca753c930eaef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38b414f3b9d44261be50a566ce05b849","IPY_MODEL_d40b1a6666bb4681a8cd62a2c0ea3d8c","IPY_MODEL_d3228e4342c74df5a7f7b024a66e9157"],"layout":"IPY_MODEL_d7fbc0b9a9f84138b1d5972a9ffae194"}},"38b414f3b9d44261be50a566ce05b849":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57c5ed4e1a084cb0a6836f710688d586","placeholder":"​","style":"IPY_MODEL_3efcc89216aa40c68a75bf79d1659e86","value":"data/train-00000-of-00002.parquet: 100%"}},"d40b1a6666bb4681a8cd62a2c0ea3d8c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec5851c5e153443984db0cb03803dfc5","max":153421593,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3ef2715cf044533a5c78c23fde4a953","value":153421593}},"d3228e4342c74df5a7f7b024a66e9157":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bed17c0b01074668b27264e2abe8143f","placeholder":"​","style":"IPY_MODEL_390bab03d1e042a78b05e09ff383e3a7","value":" 153M/153M [00:07&lt;00:00, 20.2MB/s]"}},"d7fbc0b9a9f84138b1d5972a9ffae194":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57c5ed4e1a084cb0a6836f710688d586":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3efcc89216aa40c68a75bf79d1659e86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec5851c5e153443984db0cb03803dfc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3ef2715cf044533a5c78c23fde4a953":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bed17c0b01074668b27264e2abe8143f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"390bab03d1e042a78b05e09ff383e3a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4d0ae4c3f92472c90e9f582d3520cdc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d26bfb58474b4173ad7ba48b40f7724c","IPY_MODEL_dc14534addee42a1bd48f78b13e73c84","IPY_MODEL_278831e2257c4368b46bcebec3fc78eb"],"layout":"IPY_MODEL_f16050227c4440cfb8b8a0046dcbf39a"}},"d26bfb58474b4173ad7ba48b40f7724c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ea3b908b0f84fafabee1bcb22d12762","placeholder":"​","style":"IPY_MODEL_3e03e2b078de46d59f3078fcfff6b18a","value":"data/train-00001-of-00002.parquet: 100%"}},"dc14534addee42a1bd48f78b13e73c84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_67889ae4c02544a8857f15e03af3c390","max":153206051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de7e639de1fd4d14b3df948c3508e1ce","value":153206051}},"278831e2257c4368b46bcebec3fc78eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d60e1a60764a48c8b1e128a34fc72d4e","placeholder":"​","style":"IPY_MODEL_8d742e2206744c65964c0de5e96be4b4","value":" 153M/153M [00:02&lt;00:00, 69.2MB/s]"}},"f16050227c4440cfb8b8a0046dcbf39a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ea3b908b0f84fafabee1bcb22d12762":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e03e2b078de46d59f3078fcfff6b18a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67889ae4c02544a8857f15e03af3c390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de7e639de1fd4d14b3df948c3508e1ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d60e1a60764a48c8b1e128a34fc72d4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d742e2206744c65964c0de5e96be4b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5c9ff192fbf4d3095f13ecdf18e2393":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ae0de8c0e034471bb9cf0ba582e551ec","IPY_MODEL_00e8910f42fc431cbebf7e355db87991","IPY_MODEL_973305381631464abe1c978e5fe3998e"],"layout":"IPY_MODEL_9403ebfa60244fd598d0bb37ae5557f6"}},"ae0de8c0e034471bb9cf0ba582e551ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e53713e763a4465bad2b3d05b9ff25a","placeholder":"​","style":"IPY_MODEL_ee8d4480ff8e462da2c647cb475eee88","value":"Generating train split: 100%"}},"00e8910f42fc431cbebf7e355db87991":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b89b27800e8b4090ad1f0b7093663350","max":100000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1e846d7c7a2495096be8de78e582d77","value":100000}},"973305381631464abe1c978e5fe3998e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3864769ba542427898d1556b2e165290","placeholder":"​","style":"IPY_MODEL_75bdb24d509b4929b2ae26d9829bb271","value":" 100000/100000 [00:03&lt;00:00, 39103.38 examples/s]"}},"9403ebfa60244fd598d0bb37ae5557f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e53713e763a4465bad2b3d05b9ff25a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee8d4480ff8e462da2c647cb475eee88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b89b27800e8b4090ad1f0b7093663350":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1e846d7c7a2495096be8de78e582d77":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3864769ba542427898d1556b2e165290":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75bdb24d509b4929b2ae26d9829bb271":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbd0d60cecd442499c1ad72f2e589a27":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_866e467bbec042a6bbe77201534dcfee","IPY_MODEL_d6114c9d791640fb8c766916652c7e97","IPY_MODEL_e665d19376fa44fab233b8c26a1ff74f"],"layout":"IPY_MODEL_bf7824a9765a4966a78422ec5f070703"}},"866e467bbec042a6bbe77201534dcfee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5089628c9cfa46adb27d85e29d241ce6","placeholder":"​","style":"IPY_MODEL_ad6ef57b65de47778f5152edc3dc350b","value":"tokenizer_config.json: 100%"}},"d6114c9d791640fb8c766916652c7e97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09d752425a2a4f1597456e62ce03b083","max":3594,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97a6a19c97294ea4912801e28022f99b","value":3594}},"e665d19376fa44fab233b8c26a1ff74f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7582aa6f226842ca8ff095f350c06b08","placeholder":"​","style":"IPY_MODEL_90eded1b625647a68be6cff1755abf09","value":" 3.59k/3.59k [00:00&lt;00:00, 369kB/s]"}},"bf7824a9765a4966a78422ec5f070703":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5089628c9cfa46adb27d85e29d241ce6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad6ef57b65de47778f5152edc3dc350b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09d752425a2a4f1597456e62ce03b083":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97a6a19c97294ea4912801e28022f99b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7582aa6f226842ca8ff095f350c06b08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90eded1b625647a68be6cff1755abf09":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f2f75343f454d32a121ab3262c0197a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77103bafa8bf4d5096172e5f4407532a","IPY_MODEL_4a69e9288fc74d788da9c35e104e4583","IPY_MODEL_6326ab28d39f432884a26e25e8e107c6"],"layout":"IPY_MODEL_5573eb47e4a94f89bebfd17dc158cd4d"}},"77103bafa8bf4d5096172e5f4407532a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bf19b46e09b46f594cd3987c77f6e7a","placeholder":"​","style":"IPY_MODEL_df831808664841ce9ba6059b6785712e","value":"tokenizer.json: 100%"}},"4a69e9288fc74d788da9c35e104e4583":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_67ed0674692f4afdb1c578cf2d7ce401","max":7847602,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19289a1a8a7c4d5686e1fc025e6fb5c9","value":7847602}},"6326ab28d39f432884a26e25e8e107c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_daa65961ee1e45b08a01511508468b1e","placeholder":"​","style":"IPY_MODEL_0c1fec5dd83249f4940ad946a3aef038","value":" 7.85M/7.85M [00:00&lt;00:00, 20.9MB/s]"}},"5573eb47e4a94f89bebfd17dc158cd4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bf19b46e09b46f594cd3987c77f6e7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df831808664841ce9ba6059b6785712e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67ed0674692f4afdb1c578cf2d7ce401":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19289a1a8a7c4d5686e1fc025e6fb5c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"daa65961ee1e45b08a01511508468b1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c1fec5dd83249f4940ad946a3aef038":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}